import torch
import pandas as pd
import numpy as np
import os
import shutil
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)

# --- å¯¼å…¥é…ç½®å’Œå·¥å…· ---
# å½“å¯¼å…¥ config æ—¶ï¼Œæ¨¡å‹é€‰æ‹©çš„äº¤äº’å¼æµç¨‹å·²ç»è¿è¡Œå®Œæ¯•ï¼Œå˜é‡å·²è¢«è®¾ç½®
from config import (
    DATA_PATH, CACHE_PATH, FINAL_MODEL_DIR, SEED, 
    TARGET_EMOTIONS, NUM_LABELS, DEVICE, 
    MODEL_PATH, CURRENT_CONFIG, MODEL_CHOICE
)
# å‡è®¾ cleancache.py åŒ…å« clear_folder å‡½æ•°
from cleancache import clear_folder

# æ‰“å°å½“å‰ä½¿ç”¨çš„é…ç½® (ç°åœ¨è¿™äº›å˜é‡æ˜¯åŸºäºç”¨æˆ·é€‰æ‹©æˆ–é»˜è®¤å€¼è®¾ç½®çš„)
print("\n=========================================")
# ç§»é™¤æè¿°ä¸­çš„é»˜è®¤æ ‡è®°ï¼Œè®©è¾“å‡ºæ›´æ•´æ´
display_desc = CURRENT_CONFIG['description'].replace(" (é»˜è®¤)", "")
print(f"ğŸš€ æ¨¡å‹é€‰æ‹©: {MODEL_CHOICE} ({display_desc})")
print(f"ğŸ§  æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"ğŸ’» è¿è¡Œè®¾å¤‡: {DEVICE}")
print(f"ğŸ¯ æ ‡ç­¾æ•°é‡: {NUM_LABELS} (æƒ…ç»ªç±»åˆ«)")
print(f"ğŸ“Š æ ¸å¿ƒè¶…å‚æ•°:")
for k, v in CURRENT_CONFIG.items():
    if k not in ["model_path", "description"]:
        print(f"   - {k}: {v}")
print("=========================================")

# å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


def load_data(data_path=DATA_PATH):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼Œå¼ºåˆ¶ä½¿ç”¨å®šä¹‰çš„æƒ…ç»ªæ ‡ç­¾"""
    try:
        data = pd.read_csv(data_path)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ '{data_path}' æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®è·¯å¾„ã€‚")
        # é€€å‡ºç¨‹åº
        exit()
    
    # ç­›é€‰æœ‰æ•ˆæ ‡ç­¾ï¼Œå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é˜²ä¸‡ä¸€
    data["label"] = data["label"].astype(str)
    # å¼ºåˆ¶åªä¿ç•™ TARGET_EMOTIONS ä¸­çš„æ ‡ç­¾
    data = data[data["label"].isin(TARGET_EMOTIONS)].copy() 
    data["text"] = data["text"].astype(str)

    if data.empty:
        print(f"é”™è¯¯ï¼šåœ¨ '{data_path}' ä¸­æ²¡æœ‰æ‰¾åˆ°å±äº TARGET_EMOTIONS çš„æ•°æ®ã€‚")
        exit()

    # æ•°æ®ç»Ÿè®¡
    print("\n=== æ•°æ®ç»Ÿè®¡ ===")
    print("ç­›é€‰åæ€»æ ·æœ¬æ•°:", len(data))
    
    # ä½¿ç”¨å›ºå®šé¡ºåºçš„æ ‡ç­¾ç¼–ç å™¨
    label_encoder = LabelEncoder()
    label_encoder.fit(TARGET_EMOTIONS)  # å¼ºåˆ¶æŒ‰å®šä¹‰é¡ºåºç¼–ç 

    # åˆ’åˆ†æ•°æ®é›†ï¼šä¿è¯æµ‹è¯•é›†è‡³å°‘åŒ…å«æ¯ä¸ªç±»åˆ«ä¸€ä¸ªæ ·æœ¬ï¼Œå¹¶å°è¯•åˆ†å±‚
    test_size = 0.2 # é»˜è®¤æµ‹è¯•é›†æ¯”ä¾‹
    if len(data) < NUM_LABELS * 2:
         print("è­¦å‘Šï¼šæ•°æ®é‡è¿‡å°‘ï¼Œåˆ†å±‚æŠ½æ ·å¯èƒ½å¤±è´¥æˆ–æ•ˆæœä¸ä½³ã€‚")
         test_size = 0.1

    try:
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            data["text"].tolist(),
            data["label"].tolist(),
            test_size=test_size,
            stratify=data["label"], # å°è¯•åˆ†å±‚æŠ½æ ·
            random_state=SEED
        )
    except ValueError as e:
        print(f"åˆ†å±‚æŠ½æ ·å¤±è´¥: {e}. å°è¯•éåˆ†å±‚æŠ½æ ·...")
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            data["text"].tolist(),
            data["label"].tolist(),
            test_size=test_size,
            random_state=SEED
        )

    # ç¼–ç æ ‡ç­¾
    train_labels_encoded = label_encoder.transform(train_labels)
    test_labels_encoded = label_encoder.transform(test_labels)

    print(f"\nåˆ’åˆ†ç»“æœ: è®­ç»ƒé›†={len(train_texts)}, æµ‹è¯•é›†={len(test_texts)}")
    print("æ ‡ç­¾æ˜ å°„:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

    return train_texts, test_texts, train_labels_encoded, test_labels_encoded, label_encoder


class EmotionDataset(torch.utils.data.Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç”¨äºTrainer"""
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.as_tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.as_tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


def train_and_evaluate():
    """è®­ç»ƒå’Œè¯„ä¼°æƒ…ç»ªåˆ†ç±»æ¨¡å‹çš„ä¸»æµç¨‹""" 
    # 1. åŠ è½½æ•°æ®
    train_texts, test_texts, train_labels, test_labels, label_encoder = load_data()

    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\nåˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹...")
    # ä½¿ç”¨ use_fast=False é¿å…æŸäº›ç‰¹æ®Šæ¨¡å‹çš„é—®é¢˜ï¼Œä½†é€šå¸¸ fast=True æ›´å¿«
    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, use_fast=False) 
    model = BertForSequenceClassification.from_pretrained(
        MODEL_PATH,
        num_labels=NUM_LABELS,
        # ä¼ é€’æ ‡ç­¾æ˜ å°„ç»™æ¨¡å‹é…ç½®ï¼Œä¾¿äºéƒ¨ç½²
        id2label={i: label for i, label in enumerate(label_encoder.classes_)},
        label2id={label: i for i, label in enumerate(label_encoder.classes_)}
    )

    # 3. æ•°æ®ç¼–ç 
    print("\nTokenizing æ•°æ®...")
    train_encodings = tokenizer(
        train_texts,
        truncation=True,
        padding="max_length",
        max_length=128,
    )
    test_encodings = tokenizer(
        test_texts,
        truncation=True,
        padding="max_length",
        max_length=128,
    )

    # 4. åˆ›å»ºæ•°æ®é›†
    train_dataset = EmotionDataset(train_encodings, train_labels)
    test_dataset = EmotionDataset(test_encodings, test_labels)

    # 5. è®­ç»ƒé…ç½®ï¼ˆä½¿ç”¨ config.py ä¸­çš„è¶…å‚æ•°ï¼‰
    training_args = TrainingArguments(
        output_dir=CACHE_PATH,                       # ä¸´æ—¶æ£€æŸ¥ç‚¹å’Œæ—¥å¿—ç›®å½•
        num_train_epochs=CURRENT_CONFIG["num_train_epochs"],
        per_device_train_batch_size=CURRENT_CONFIG["per_device_train_batch_size"],
        per_device_eval_batch_size=32,
        learning_rate=CURRENT_CONFIG["learning_rate"],
        weight_decay=CURRENT_CONFIG["weight_decay"],
        warmup_ratio=CURRENT_CONFIG["warmup_ratio"],
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_weighted",         # æŒ‰åŠ æƒ F1 é€‰æ‹©æœ€ä½³æ¨¡å‹
        greater_is_better=True,
        logging_dir=f'{CACHE_PATH}/logs',            # æŒ‡å®šæ—¥å¿—ç›®å½•
        logging_steps=50,
        seed=SEED,
        fp16=torch.cuda.is_available(),              # å¦‚æœå¯ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨æ··åˆç²¾åº¦
        report_to="none"                             # ç¦ç”¨å¤–éƒ¨æŠ¥å‘Š
    )

    # 6. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        
        # ä½¿ç”¨ label_encoder.classes_ è·å–æ­£ç¡®çš„æ ‡ç­¾åç§°é¡ºåº
        report = classification_report(
            labels, preds,
            target_names=label_encoder.classes_,
            output_dict=True,
            zero_division=0 # å¤„ç†æŸä¸ªç±»åˆ«åœ¨é¢„æµ‹æˆ–çœŸå®æ ‡ç­¾ä¸­éƒ½æ²¡æœ‰å‡ºç°çš„æƒ…å†µ
        )
        
        # è¿”å› Trainer éœ€è¦çš„æŒ‡æ ‡
        return {
            "accuracy": report["accuracy"],
            "f1_weighted": report["weighted avg"]["f1-score"],
            "precision_weighted": report["weighted avg"]["precision"],
            "recall_weighted": report["weighted avg"]["recall"],
        }

    # 7. è®­ç»ƒ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
        callbacks=[EarlyStoppingCallback(
            early_stopping_patience=CURRENT_CONFIG["early_stopping_patience"] # ä»é…ç½®ä¸­è·å–æ—©åœè€å¿ƒå€¼
        )]
    )

    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # 8. æœ€ç»ˆè¯„ä¼° (åŠ è½½æœ€å¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°)
    print("\n=== æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½ (ä½¿ç”¨æœ€ä½³æ¨¡å‹) ===")
    eval_results = trainer.evaluate(test_dataset)
    print(f"è¯„ä¼°ç»“æœ: {eval_results}")

    # è·å–è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    predictions = trainer.predict(test_dataset)
    y_pred = np.argmax(predictions.predictions, axis=1)
    print(classification_report(
        test_labels, # ä½¿ç”¨åŸå§‹ç¼–ç çš„ test_labels
        y_pred,
        target_names=label_encoder.classes_, # ä½¿ç”¨æ­£ç¡®çš„æ ‡ç­¾åç§°
        digits=4
    ))

    # 9. ä¿å­˜æ¨¡å‹å’Œé…ç½®
    os.makedirs(FINAL_MODEL_DIR, exist_ok=True)
    print(f"\nä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {FINAL_MODEL_DIR}...")
    trainer.save_model(FINAL_MODEL_DIR) # ä¿å­˜æœ€ä½³æ¨¡å‹ã€tokenizeré…ç½®ã€è®­ç»ƒçŠ¶æ€ç­‰
    tokenizer.save_pretrained(FINAL_MODEL_DIR) # ç¡®ä¿ tokenizer ä¹Ÿä¿å­˜

    # ä¿å­˜æ ‡ç­¾æ˜ å°„
    label_mapping_path = os.path.join(FINAL_MODEL_DIR, "label_mapping.json")
    print(f"ä¿å­˜æ ‡ç­¾æ˜ å°„åˆ° {label_mapping_path}...")
    with open(label_mapping_path, "w", encoding="utf-8") as f:
        json.dump({
            "id2label": {str(i): label for i, label in enumerate(label_encoder.classes_)},
            "label2id": {label: i for i, label in enumerate(label_encoder.classes_)}
        }, f, ensure_ascii=False, indent=2)

    print(f"\næ¨¡å‹å’Œé…ç½®å·²ä¿å­˜åˆ° {FINAL_MODEL_DIR}")


if __name__ == "__main__":
    train_and_evaluate()
    print("æ˜¯å¦æ¸…ç©ºè®­ç»ƒç¼“å­˜æ–‡ä»¶å¤¹ results_18emoï¼Ÿ(y/n)")
    # ä½¿ç”¨ input() æ¥è·å–ç”¨æˆ·è¾“å…¥
    user_input = input().lower().strip() 
    if user_input == "y":
        clear_folder(CACHE_PATH)
        print("ç¼“å­˜å·²æ¸…ç©º")
    else:
        print(f"ç¼“å­˜æœªæ¸…ç©ºï¼Œä¿å­˜åœ¨:{CACHE_PATH}")
    print("è®­ç»ƒå®Œæˆï¼")